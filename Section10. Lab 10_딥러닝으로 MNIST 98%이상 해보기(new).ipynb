{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax classifier for MNIST - 88% - 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  6.04373710416\n",
      "Epoch:  2 cost:  1.84058975783\n",
      "Epoch:  3 cost:  1.12709356107\n",
      "Epoch:  4 cost:  0.871442506855\n",
      "Epoch:  5 cost:  0.736369967704\n",
      "Epoch:  6 cost:  0.650684807057\n",
      "Epoch:  7 cost:  0.59128416037\n",
      "Epoch:  8 cost:  0.547295734313\n",
      "Epoch:  9 cost:  0.513573798293\n",
      "Epoch:  10 cost:  0.486587200815\n",
      "Epoch:  11 cost:  0.464437867593\n",
      "Epoch:  12 cost:  0.445648577484\n",
      "Epoch:  13 cost:  0.429314804253\n",
      "Epoch:  14 cost:  0.415908890881\n",
      "Epoch:  15 cost:  0.404083920609\n",
      "Accuracy:  0.8998\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvxAUIQfCIrwA0H9P83hV9atsG3zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1214c99b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# hypothesis - sosftmax\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # image\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN for MNIST(layer3) - 94% - 95%\n",
    "- AdamOptimizer 사용\n",
    "- learning_rate = 0.001\n",
    "- 왜 softmax를 하던 안하던 결과에 차이가 없지???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  172.768232644\n",
      "Epoch:  2 cost:  40.8111541497\n",
      "Epoch:  3 cost:  25.4378825678\n",
      "Epoch:  4 cost:  17.3204087288\n",
      "Epoch:  5 cost:  12.6172706886\n",
      "Epoch:  6 cost:  9.29916432367\n",
      "Epoch:  7 cost:  6.8585627384\n",
      "Epoch:  8 cost:  5.06524947628\n",
      "Epoch:  9 cost:  3.81179889946\n",
      "Epoch:  10 cost:  2.80760513039\n",
      "Epoch:  11 cost:  2.10989342697\n",
      "Epoch:  12 cost:  1.6303805737\n",
      "Epoch:  13 cost:  1.16854780602\n",
      "Epoch:  14 cost:  0.995287794124\n",
      "Epoch:  15 cost:  0.737184109701\n",
      "Accuracy:  0.9426\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDyo4DHy20dVGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126419940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer2\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# layer3\n",
    "W3 = tf.Variable(tf.random_normal([256, nb_classes]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')\n",
    "hypothesis = tf.matmul(layer2, W3) + b3\n",
    "# 왜 softmax를 하던 안하던 결과에 차이가 없지???\n",
    "# hytpothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # image\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier for MNIST - 97%\n",
    "- 초기 cost부너 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  0.298092651848\n",
      "Epoch:  2 cost:  0.116435956213\n",
      "Epoch:  3 cost:  0.0758872684684\n",
      "Epoch:  4 cost:  0.0534843009998\n",
      "Epoch:  5 cost:  0.0406662059512\n",
      "Epoch:  6 cost:  0.0304488536176\n",
      "Epoch:  7 cost:  0.0247021331512\n",
      "Epoch:  8 cost:  0.0175976976395\n",
      "Epoch:  9 cost:  0.0189658484988\n",
      "Epoch:  10 cost:  0.0143075150817\n",
      "Epoch:  11 cost:  0.0114467684843\n",
      "Epoch:  12 cost:  0.0116579217928\n",
      "Epoch:  13 cost:  0.0109406362631\n",
      "Epoch:  14 cost:  0.0113560962398\n",
      "Epoch:  15 cost:  0.0103810287363\n",
      "Accuracy:  0.9767\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d5fdf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# layer3\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')\n",
    "hypothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # image\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout for MNIST - 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  0.361044012715\n",
      "Epoch:  2 cost:  0.140113720785\n",
      "Epoch:  3 cost:  0.101993900261\n",
      "Epoch:  4 cost:  0.0828732445853\n",
      "Epoch:  5 cost:  0.0681731744187\n",
      "Epoch:  6 cost:  0.0593349973121\n",
      "Epoch:  7 cost:  0.0526469905332\n",
      "Epoch:  8 cost:  0.0471436159791\n",
      "Epoch:  9 cost:  0.0437287325037\n",
      "Epoch:  10 cost:  0.0380750312358\n",
      "Epoch:  11 cost:  0.0379490000928\n",
      "Epoch:  12 cost:  0.0352705229692\n",
      "Epoch:  13 cost:  0.0295261084982\n",
      "Epoch:  14 cost:  0.0306189534758\n",
      "Epoch:  15 cost:  0.028714167354\n",
      "Accuracy:  0.9825\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"W1111\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"W2222\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "\n",
    "# layer3 - hypothesis\n",
    "W3 = tf.get_variable(\"W3333\", shape=[512, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')\n",
    "hypothesis = tf.matmul(layer2, W3) + b3\n",
    "# 왜 softmax를 하던 안하던 결과에 차이가 없지???\n",
    "# hytpothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "    \n",
    "    # image\n",
    "#     r = random.randint(0, mnist.test.num_examples -1)\n",
    "#     print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "#     print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "#     plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimizer \n",
    "- GradientDescentOptimizer\n",
    "- AdamOptimizer: cost가 가장 빨리 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

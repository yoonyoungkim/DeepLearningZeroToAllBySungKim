{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax classifier for MNIST - 88% - 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  6.41904358799\n",
      "Epoch:  2 cost:  1.94121612679\n",
      "Epoch:  3 cost:  1.24704371658\n",
      "Epoch:  4 cost:  0.970912621292\n",
      "Epoch:  5 cost:  0.819083080888\n",
      "Epoch:  6 cost:  0.721380089494\n",
      "Epoch:  7 cost:  0.652921914512\n",
      "Epoch:  8 cost:  0.600314971452\n",
      "Epoch:  9 cost:  0.559859586548\n",
      "Epoch:  10 cost:  0.526986587075\n",
      "Epoch:  11 cost:  0.499599382932\n",
      "Epoch:  12 cost:  0.476694794649\n",
      "Epoch:  13 cost:  0.457411459915\n",
      "Epoch:  14 cost:  0.440621560785\n",
      "Epoch:  15 cost:  0.426122595018\n",
      "Accuracy:  0.8981\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1266b4e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# hypothesis - sosftmax\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # image\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN for MNIST(layer3) - 94% - 95%\n",
    "- AdamOptimizer 사용\n",
    "- learning_rate = 0.001\n",
    "- 왜 softmax를 하던 안하던 결과에 차이가 없지???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  181.318326721\n",
      "Epoch:  2 cost:  40.8584132333\n",
      "Epoch:  3 cost:  25.4827039227\n",
      "Epoch:  4 cost:  17.8341236912\n",
      "Epoch:  5 cost:  13.0117311823\n",
      "Epoch:  6 cost:  9.74428313293\n",
      "Epoch:  7 cost:  7.33284847697\n",
      "Epoch:  8 cost:  5.50462426254\n",
      "Epoch:  9 cost:  4.17966615839\n",
      "Epoch:  10 cost:  3.20381274673\n",
      "Epoch:  11 cost:  2.44361346857\n",
      "Epoch:  12 cost:  1.96104822454\n",
      "Epoch:  13 cost:  1.45582627394\n",
      "Epoch:  14 cost:  1.21914978391\n",
      "Epoch:  15 cost:  0.924772244064\n",
      "Accuracy:  0.9475\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkjYYYYg2ZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124393b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer2\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# layer3\n",
    "W3 = tf.Variable(tf.random_normal([256, nb_classes]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')\n",
    "logits = tf.matmul(layer2, W3) + b3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "# 왜 softmax를 하던 안하던 결과에 차이가 없지???\n",
    "# hytpothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # image\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier for MNIST - 97%\n",
    "- 초기 cost부너 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  0.305146069296\n",
      "Epoch:  2 cost:  0.109111193981\n",
      "Epoch:  3 cost:  0.0698609856415\n",
      "Epoch:  4 cost:  0.0516840667184\n",
      "Epoch:  5 cost:  0.0394544425581\n",
      "Epoch:  6 cost:  0.0297543584817\n",
      "Epoch:  7 cost:  0.0220657719767\n",
      "Epoch:  8 cost:  0.0186972466758\n",
      "Epoch:  9 cost:  0.014700588786\n",
      "Epoch:  10 cost:  0.0153706266983\n",
      "Epoch:  11 cost:  0.0138641548725\n",
      "Epoch:  12 cost:  0.0117197170617\n",
      "Epoch:  13 cost:  0.011975327668\n",
      "Epoch:  14 cost:  0.00740573689125\n",
      "Epoch:  15 cost:  0.0102818267622\n",
      "Accuracy:  0.9761\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127196da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# layer3\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')\n",
    "logits = tf.matmul(layer2, W3) + b3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "# 왜 softmax를 하던 안하던 결과에 차이가 없지???\n",
    "# hytpothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    # image\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout for MNIST - 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 cost:  0.353601245365\n",
      "Epoch:  2 cost:  0.14098527001\n",
      "Epoch:  3 cost:  0.103723389892\n",
      "Epoch:  4 cost:  0.0846415662935\n",
      "Epoch:  5 cost:  0.0685521420879\n",
      "Epoch:  6 cost:  0.0600751236742\n",
      "Epoch:  7 cost:  0.0553746205175\n",
      "Epoch:  8 cost:  0.0482904526494\n",
      "Epoch:  9 cost:  0.043213649971\n",
      "Epoch:  10 cost:  0.0389204646916\n",
      "Epoch:  11 cost:  0.037644170408\n",
      "Epoch:  12 cost:  0.0352996131295\n",
      "Epoch:  13 cost:  0.0319177263592\n",
      "Epoch:  14 cost:  0.0329664034453\n",
      "Epoch:  15 cost:  0.0293174608787\n",
      "Accuracy:  0.9824\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# MNIST Dataset 가져오기\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# shape = 28 * 28\n",
    "nb_classes = 10\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# layer1\n",
    "W1 = tf.get_variable(\"W111\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "layer1 = tf.nn.dropout(layer1, keep_prob=keep_prob)\n",
    "\n",
    "# layer2\n",
    "W2 = tf.get_variable(\"W222\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=keep_prob)\n",
    "\n",
    "# layer3 - hypothesis\n",
    "W3 = tf.get_variable(\"W333\", shape=[512, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]), name='bias3')\n",
    "logits = tf.matmul(layer2, W3) + b3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "# 왜 softmax를 하던 안하던 결과에 차이가 없지???\n",
    "# hytpothesis = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "# cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(predicted, tf.argmax(Y, 1)) # 에측값이 참이면 True, 아니면 False\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# batch로 작업\n",
    "training_epochs = 15 # 전체 데이터 셋을 모두 한번씩 학습시키는 것을 1 epoch이라고 한다\n",
    "batch_size = 100 # 한번에 읽어들일 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost= 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch: ', epoch+1, 'cost: ', avg_cost)\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "    \n",
    "    # image\n",
    "#     r = random.randint(0, mnist.test.num_examples -1)\n",
    "#     print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "#     print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r+1]}))\n",
    "#     plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimizer \n",
    "- GradientDescentOptimizer\n",
    "- AdamOptimizer: cost가 가장 빨리 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

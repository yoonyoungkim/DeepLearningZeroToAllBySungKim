{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.737463 [[-0.90722913]\n",
      " [-0.35967317]]\n",
      "100 0.701239 [[-0.46812993]\n",
      " [-0.17435254]]\n",
      "200 0.696625 [[-0.29391909]\n",
      " [-0.13682237]]\n",
      "300 0.694672 [[-0.18750285]\n",
      " [-0.10354938]]\n",
      "400 0.693824 [[-0.12071116]\n",
      " [-0.07585615]]\n",
      "500 0.69345 [[-0.07833767]\n",
      " [-0.05437431]]\n",
      "600 0.693284 [[-0.05119434]\n",
      " [-0.03839253]]\n",
      "700 0.693209 [[-0.03365513]\n",
      " [-0.02681617]]\n",
      "800 0.693175 [[-0.02223524]\n",
      " [-0.01858176]]\n",
      "900 0.69316 [[-0.01475102]\n",
      " [-0.01279931]]\n",
      "1000 0.693153 [[-0.0098191 ]\n",
      " [-0.00877647]]\n",
      "1100 0.69315 [[-0.00655418]\n",
      " [-0.0059972 ]]\n",
      "1200 0.693148 [[-0.00438464]\n",
      " [-0.00408709]]\n",
      "1300 0.693148 [[-0.00293854]\n",
      " [-0.00277958]]\n",
      "1400 0.693147 [[-0.00197223]\n",
      " [-0.00188732]]\n",
      "1500 0.693147 [[-0.00132522]\n",
      " [-0.00127985]]\n",
      "1600 0.693147 [[-0.00089129]\n",
      " [-0.00086706]]\n",
      "1700 0.693147 [[-0.0005999 ]\n",
      " [-0.00058695]]\n",
      "1800 0.693147 [[-0.000404  ]\n",
      " [-0.00039708]]\n",
      "1900 0.693147 [[-0.00027223]\n",
      " [-0.00026853]]\n",
      "2000 0.693147 [[-0.00018349]\n",
      " [-0.00018151]]\n",
      "2100 0.693147 [[-0.0001237 ]\n",
      " [-0.00012265]]\n",
      "2200 0.693147 [[ -8.34187958e-05]\n",
      " [ -8.28561388e-05]]\n",
      "2300 0.693147 [[ -5.62599307e-05]\n",
      " [ -5.59595574e-05]]\n",
      "2400 0.693147 [[ -3.79493904e-05]\n",
      " [ -3.77875913e-05]]\n",
      "2500 0.693147 [[ -2.55978157e-05]\n",
      " [ -2.55105206e-05]]\n",
      "2600 0.693147 [[ -1.72680684e-05]\n",
      " [ -1.72195159e-05]]\n",
      "2700 0.693147 [[ -1.16473484e-05]\n",
      " [ -1.16226365e-05]]\n",
      "2800 0.693147 [[ -7.85202519e-06]\n",
      " [ -7.83774522e-06]]\n",
      "2900 0.693147 [[ -5.29647605e-06]\n",
      " [ -5.28815690e-06]]\n",
      "3000 0.693147 [[ -3.57389945e-06]\n",
      " [ -3.57302906e-06]]\n",
      "3100 0.693147 [[ -2.41160910e-06]\n",
      " [ -2.41073872e-06]]\n",
      "3200 0.693147 [[ -1.60843274e-06]\n",
      " [ -1.60756235e-06]]\n",
      "3300 0.693147 [[ -1.11222027e-06]\n",
      " [ -1.11134989e-06]]\n",
      "3400 0.693147 [[ -7.44159593e-07]\n",
      " [ -7.43289206e-07]]\n",
      "3500 0.693147 [[ -4.89349645e-07]\n",
      " [ -4.88479259e-07]]\n",
      "3600 0.693147 [[ -3.13515329e-07]\n",
      " [ -3.12644943e-07]]\n",
      "3700 0.693147 [[ -2.25598143e-07]\n",
      " [ -2.24727756e-07]]\n",
      "3800 0.693147 [[ -1.76424123e-07]\n",
      " [ -1.75553737e-07]]\n",
      "3900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4100 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4200 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4300 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4400 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4500 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4600 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4700 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4800 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "4900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5100 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5200 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5300 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5400 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5500 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5600 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5700 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5800 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "5900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6100 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6200 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6300 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6400 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6500 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6600 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6700 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6800 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "6900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7100 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7200 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7300 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7400 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7500 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7600 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7700 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7800 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "7900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8100 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8200 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8300 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8400 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8500 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8600 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8700 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8800 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "8900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9100 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9200 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9300 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9400 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9500 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9600 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9700 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9800 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "9900 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "10000 0.693147 [[ -1.33210591e-07]\n",
      " [ -1.32340205e-07]]\n",
      "\n",
      "hypothesis:  [[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]] \n",
      "Correct:  [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# hypothersis - sigmoid function\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "# cost\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step%100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nhypothesis: ', h, '\\nCorrect: ', c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logical reg not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.71171 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "100 0.693512 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "200 0.693086 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "300 0.692587 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "400 0.691946 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "500 0.691083 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "600 0.689898 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "700 0.688274 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "800 0.686099 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "900 0.683289 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1000 0.679805 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1100 0.675634 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1200 0.670763 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1300 0.665159 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1400 0.658772 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1500 0.651554 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1600 0.643498 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1700 0.634663 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1800 0.625197 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "1900 0.615328 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2000 0.605331 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2100 0.595484 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2200 0.586023 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2300 0.577121 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2400 0.56888 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2500 0.561346 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2600 0.554514 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2700 0.548356 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2800 0.542821 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "2900 0.537852 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3000 0.533392 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3100 0.529384 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3200 0.525774 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3300 0.522515 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3400 0.519563 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3500 0.516883 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3600 0.514439 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3700 0.512203 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3800 0.51015 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "3900 0.508256 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4000 0.506503 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4100 0.504872 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4200 0.503348 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4300 0.501917 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4400 0.500566 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4500 0.499285 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4600 0.49806 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4700 0.496882 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4800 0.495742 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "4900 0.494627 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5000 0.493528 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5100 0.492432 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5200 0.491328 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5300 0.4902 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5400 0.489031 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5500 0.487799 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5600 0.486479 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5700 0.485036 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5800 0.483427 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "5900 0.481592 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6000 0.479453 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6100 0.476902 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6200 0.473793 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6300 0.469927 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6400 0.465039 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6500 0.45879 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6600 0.450763 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6700 0.440485 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6800 0.427465 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "6900 0.411277 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7000 0.391682 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7100 0.368766 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7200 0.343022 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7300 0.315343 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7400 0.286895 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7500 0.258895 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7600 0.232384 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7700 0.208075 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7800 0.186326 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "7900 0.167198 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8000 0.150556 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8100 0.13616 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8200 0.123728 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8300 0.112984 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8400 0.103673 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8500 0.0955731 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8600 0.0884944 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8700 0.0822779 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8800 0.0767913 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "8900 0.0719247 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9000 0.0675871 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9100 0.0637029 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9200 0.0602091 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9300 0.0570532 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9400 0.0541911 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9500 0.0515857 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9600 0.0492056 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9700 0.0470239 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9800 0.0450178 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "9900 0.0431677 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "10000 0.0414569 [[-1.65148759]\n",
      " [-1.01460886]]\n",
      "\n",
      "hypothesis:  [[ 0.02465636]\n",
      " [ 0.96394867]\n",
      " [ 0.96394813]\n",
      " [ 0.06520422]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "# hypothersis - sigmoid function\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "# cost\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step%100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nhypothesis: ', h, '\\nCorrect: ', c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1111 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "100 0.693566 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "200 0.670791 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "300 0.647313 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "400 0.621013 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "500 0.589994 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "600 0.55303 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "700 0.509904 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "800 0.461696 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "900 0.410799 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1000 0.360339 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1100 0.313177 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1200 0.271154 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1300 0.234955 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1400 0.204413 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1500 0.178911 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1600 0.157689 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1700 0.14 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1800 0.125192 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "1900 0.112719 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2000 0.102142 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2100 0.093107 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2200 0.0853341 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2300 0.0786001 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2400 0.0727273 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2500 0.0675732 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2600 0.063023 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2700 0.0589837 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2800 0.0553793 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "2900 0.0521476 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3000 0.0492367 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3100 0.0466039 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3200 0.0442134 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3300 0.0420347 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3400 0.0400424 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3500 0.0382146 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3600 0.0365328 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3700 0.0349809 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3800 0.033545 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "3900 0.0322133 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4000 0.0309751 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4100 0.0298215 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4200 0.0287444 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4300 0.0277366 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4400 0.026792 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4500 0.0259051 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4600 0.0250709 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4700 0.024285 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4800 0.0235434 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "4900 0.0228427 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5000 0.0221797 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5100 0.0215516 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5200 0.0209557 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5300 0.0203896 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5400 0.0198514 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5500 0.019339 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5600 0.0188508 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5700 0.0183849 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5800 0.0179402 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "5900 0.017515 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6000 0.0171083 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6100 0.016719 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6200 0.0163458 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6300 0.015988 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6400 0.0156445 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6500 0.0153145 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6600 0.0149974 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6700 0.0146924 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6800 0.0143988 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "6900 0.014116 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7000 0.0138435 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7100 0.0135807 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7200 0.013327 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7300 0.0130822 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7400 0.0128457 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7500 0.0126171 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7600 0.0123959 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7700 0.0121821 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7800 0.011975 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "7900 0.0117745 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8000 0.0115802 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8100 0.0113918 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8200 0.0112092 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8300 0.0110319 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8400 0.0108599 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8500 0.0106929 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8600 0.0105307 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8700 0.010373 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8800 0.0102198 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "8900 0.0100707 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9000 0.00992576 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9100 0.00978468 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9200 0.00964733 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9300 0.00951358 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9400 0.00938332 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9500 0.00925632 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9600 0.00913259 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9700 0.00901195 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9800 0.00889425 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "9900 0.00877947 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "10000 0.0086674 [[ 0.86401039]\n",
      " [ 0.44715267]]\n",
      "\n",
      "hypothesis:  [[ 0.00856852]\n",
      " [ 0.99273032]\n",
      " [ 0.99002391]\n",
      " [ 0.00870365]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "# hypothersis - sigmoid function\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "# cost\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step%100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nhypothesis: ', h, '\\nCorrect: ', c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "\n",
    "# hypothersis - sigmoid function\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "# cost\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# measure\n",
    "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step%100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nhypothesis: ', h, '\\nCorrect: ', c, \"\\nAccuracy: \", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
